---
title: Research
layout: article
---

Our research interests include **robot audition**, **sound-aware multimodal LLMs**, and **sound-based embodied AI**. 
Ultimately, our goal is to enable robots to understand all real-world sensory information (vision, hearing, touch, etc.) and take appropriate actions accordingly.

- **Robot audition** encompasses sound source localization, speech separation, automatic speech recognition, and robust operation in complex real-world environments.

- **Sound-aware multimodal LLMs** are capable of understanding and reasoning about the world by leveraging auditory and other sensory modalities.

- **Sound-based embodied AI** focuses on enabling agents to act based on their understanding and reasoning about the multimodal sensory world.


### Robot audition

Robot audition refers to the ability of robots to perceive and understand sounds, including key tasks such as sound source localization, speech separation, and automatic speech recognition (ASR).
Our research focuses on developing robust robot audition systems that can operate in complex real-world environments, where:

- Multiple types of sounds may occur simultaneously from different sources,
- Both sound sources and the robot itself may be in motion,
- Background noise is present, and
- Sound sources may even be occluded by obstacles, making them invisible to visual sensors.

The goal is to enable robots to accurately interpret auditory information and interact intelligently, even in such dynamic and challenging conditions.

<p align="center">
  <img src="/assets/images/research/Robot_audition_in_complex_env.png" alt="Robot_Audition" width="85%">
</p>

<!-- Robot Audition은 로봇이 소리를 인식하고 이해할 수 있도록 하는 기술로, 주요 연구 분야는 Sound Source Localization (소리의 위치 추정), Speech Separation (화자의 음성 분리), Automatic Speech Recognition (음성 인식) 등을 포함합니다.
특히 우리는 다음과 같은 실세계의 복잡한 환경에서도 이러한 기술이 안정적으로 동작할 수 있도록 연구해왔습니다:
- 다양한 종류의 소리가 여러 음원에서 동시에 발생하는 환경
- 음원과 로봇이 모두 움직일 수 있는 동적 상황
- 배경 소음이 존재하며
- 심지어 음원이 장애물에 가려 시각적으로는 보이지 않는 상황
이러한 어려운 조건에서도 소리만을 기반으로 정확하게 환경을 인식하고 이해할 수 있는 기술을 개발하는 것이 주요 목표입니다. -->

### Sound-Aware Robot Intelligence

Robot intelligence has primarily relied on vision-based perception and reasoning. However, many real-world situations cannot be perceived visually alone. For instance:

- Events like a doorbell ringing, an alarm sound, or someone falling can only be detected through sound.

In such cases, auditory information becomes essential for robots to perceive the environment and make appropriate decisions.
Our research on Sound-Aware Robot Intelligence (including **Sound-aware multimodal LLMs** and **Sound-based embodied AI**) aims to build intelligent systems that leverage sound as a core modality, effectively integrating it with vision, touch, and other sensory inputs to enable multimodal perception, reasoning, and action in real-world scenarios.

<p align="center">
  <img src="/assets/images/research/Sound_based_LLM.jpg" alt="SB_LLM" width="50%">
</p>


<!-- 
기존의 로봇 지능은 주로 비전(Vision)에 의존한 인식과 추론에 기반해 있지만, 실제 환경에서는 시각만으로는 인지할 수 없는 상황이 매우 많습니다. 예를 들어,
- 초인종, 벨소리, 사람의 낙상 소리 등은 소리를 통해서만 탐지 가능한 사건입니다.
이처럼 청각 정보가 필수적인 상황에서 로봇이 적절한 판단과 행동을 하기 위해서는, 소리를 포함한 멀티모달 정보를 통합적으로 처리할 수 있는 지능이 필요합니다.
우리는 Sound-Aware Robot Intelligence를 통해, 청각 정보를 **다른 감각 정보(시각, 촉각 등)**와 결합하고, 그에 기반해 환경을 이해하고 적절하게 행동하는 로봇 지능을 연구합니다. -->